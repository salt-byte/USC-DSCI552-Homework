{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "454c809c",
   "metadata": {},
   "source": [
    "# DSCI 552 — Homework 3\n",
    "**Student:** Yudie Deng  \n",
    "**GitHub Username:** _yudiedeng_  \n",
    "**USC ID:** _7162812062_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f161a2",
   "metadata": {},
   "source": [
    "## 1. Time Series Classification — Part 1: Feature Creation / Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5364a4",
   "metadata": {},
   "source": [
    "### 1(a) Data\n",
    "Place the unzipped **AReM** folders under `../data/AReM/`. Each subfolder (e.g., `bending1`) contains multiple instance files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b24fa91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#i find that some datasets are not in the correct format, so i need to modify the code to load the data\n",
    "#i use the chatgpt to help me modify the code(prompt: i want to load the data from the csv file, and the data is not in the correct format, so i need to modify the code to load the data)\n",
    "import os, glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "EXPECTED_COLS = 6\n",
    "\n",
    "def load_arem_super_simple(base_dir):\n",
    "    X, y_str, lengths, problems = [], [], [], []\n",
    "\n",
    "    for cls in sorted(os.listdir(base_dir)):\n",
    "        cls_dir = os.path.join(base_dir, cls)\n",
    "        if not os.path.isdir(cls_dir):\n",
    "            continue\n",
    "\n",
    "        for fp in sorted(glob.glob(os.path.join(cls_dir, \"*.csv\"))):\n",
    "            rel = f\"{cls}/{os.path.basename(fp)}\"\n",
    "            try:\n",
    "                # try use comma\n",
    "                df = pd.read_csv(fp, comment='#', header=None)\n",
    "                \n",
    "                # try use space\n",
    "                if df.shape[1] < EXPECTED_COLS:\n",
    "                    df = pd.read_csv(fp, comment='#', header=None, sep=r'\\s+')\n",
    "                \n",
    "\n",
    "                # check final columns\n",
    "                if df.shape[1] < EXPECTED_COLS:\n",
    "                    problems.append({\"file\": rel, \"reason\": f\"only {df.shape[1]} columns after all attempts\"})\n",
    "                    continue\n",
    "                    \n",
    "                # take last 6 columns\n",
    "                df = df.iloc[:, -EXPECTED_COLS:]\n",
    "\n",
    "                # convert to numeric and drop all-NaN rows\n",
    "                df = df.apply(pd.to_numeric, errors=\"coerce\").dropna(how=\"all\")\n",
    "\n",
    "                if df.shape[0] == 0:\n",
    "                    problems.append({\"file\": rel, \"reason\": \"empty after cleaning\"})\n",
    "                    continue\n",
    "\n",
    "                X.append(df.to_numpy(dtype=float))\n",
    "                y_str.append(cls)\n",
    "                lengths.append(df.shape[0])\n",
    "\n",
    "            except Exception as e:\n",
    "                problems.append({\"file\": rel, \"reason\": str(e)})\n",
    "\n",
    "    if not X:\n",
    "        raise RuntimeError(\"No valid samples loaded\")\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(y_str)\n",
    "    class_names = list(le.classes_)\n",
    "    lengths = np.array(lengths)\n",
    "\n",
    "\n",
    "    if problems:\n",
    "        print(\"\\n[issues] Problematic files:\")\n",
    "        for p in problems:\n",
    "            print(f\" - {p['file']}: {p['reason']}\")\n",
    "\n",
    "    return X, y, class_names, lengths, problems\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780cdddc",
   "metadata": {},
   "source": [
    "### 1(b) Train/Test Split by Folder Rules\n",
    "- Test: datasets 1 & 2 in `bending1` and `bending2`; datasets 1, 2, 3 in other folders.\n",
    "- Train: remaining datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5a4cd0ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 69, Test samples: 19\n",
      "Class names: [np.str_('bending1'), np.str_('bending2'), np.str_('cycling'), np.str_('lying'), np.str_('sitting'), np.str_('standing'), np.str_('walking')]\n",
      "Train length range: 479 to 480\n",
      "Test length range: 480 to 480\n",
      "\n",
      " All train/test files successfully loaded!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train, y_train, class_names, len_train, probs_train = load_arem_super_simple(\"../data/AReM_split/train/\")\n",
    "X_test,  y_test,  _,           len_test,  probs_test  = load_arem_super_simple(\"../data/AReM_split/test/\")\n",
    "\n",
    "print(f\"Train samples: {len(X_train)}, Test samples: {len(X_test)}\")\n",
    "print(\"Class names:\", class_names)\n",
    "print(\"Train length range:\", len_train.min(), \"to\", len_train.max())\n",
    "print(\"Test length range:\", len_test.min(), \"to\", len_test.max())\n",
    "\n",
    "# print problematic files\n",
    "if probs_train or probs_test:\n",
    "    print(\"\\n[Problem Files]\")\n",
    "    for p in probs_train + probs_test:\n",
    "        print(f\" - {p['file']}: {p['reason']}\")\n",
    "else:\n",
    "    print(\"\\n All train/test files successfully loaded!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6913938",
   "metadata": {},
   "source": [
    "### 1(c) Feature Extraction\n",
    "**Common time-domain features** (not exhaustive): minimum, maximum, mean, median, standard deviation, variance, range, interquartile range (IQR), first quartile (Q1), third quartile (Q3), skewness, kurtosis, energy, zero-crossing rate, mean absolute deviation, peak-to-peak amplitude.\n",
    "\n",
    "We compute: minimum, maximum, mean, median, standard deviation, Q1, Q3 for each of the six series per instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412de8ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train feature table:\n",
      "   Instance   min1   max1      mean1  median1      std1  1st_quart1  \\\n",
      "0         1  35.00  47.40  43.954500    44.33  1.557210       43.00   \n",
      "1         2  33.00  47.75  42.179812    43.50  3.666840       39.15   \n",
      "2         3  33.00  45.75  41.678063    41.75  2.241152       41.33   \n",
      "3         4  37.00  48.00  43.454958    43.25  1.384653       42.50   \n",
      "4         5  36.25  48.00  43.969125    44.50  1.616677       43.31   \n",
      "\n",
      "   3rd_quart1  min2  max2  ...      std5  1st_quart5  3rd_quart5  min6  max6  \\\n",
      "0       45.00   0.0  1.70  ...  1.997520     35.3625       36.50   0.0  1.79   \n",
      "1       45.00   0.0  3.00  ...  3.845436     30.4575       36.33   0.0  2.18   \n",
      "2       42.75   0.0  2.83  ...  2.408514     28.4575       31.25   0.0  1.79   \n",
      "3       45.00   0.0  1.58  ...  2.486268     22.2500       24.00   0.0  5.26   \n",
      "4       44.67   0.0  1.50  ...  3.314843     20.5000       23.75   0.0  2.96   \n",
      "\n",
      "      mean6  median6      std6  1st_quart6  3rd_quart6  \n",
      "0  0.493292     0.43  0.512971        0.00        0.94  \n",
      "1  0.613521     0.50  0.523771        0.00        1.00  \n",
      "2  0.383292     0.43  0.388759        0.00        0.50  \n",
      "3  0.679646     0.50  0.621885        0.43        0.87  \n",
      "4  0.555312     0.49  0.487318        0.00        0.83  \n",
      "\n",
      "[5 rows x 43 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# i told the chatgpt that i want a big loop outside and a small loop inside to help me use numpy to calculate the features\n",
    "def extract_time_features_with_instance(X):\n",
    "    all_features = []\n",
    "    for idx, sample in enumerate(X, start=1):\n",
    "        row_feats = [idx]  # Instance \n",
    "        for j in range(sample.shape[1]):\n",
    "            x = sample[:, j]\n",
    "            row_feats.extend([\n",
    "                np.min(x), np.max(x),\n",
    "                np.mean(x), np.median(x),\n",
    "                np.std(x),\n",
    "                np.percentile(x, 25), np.percentile(x, 75)\n",
    "            ])\n",
    "        all_features.append(row_feats)\n",
    "\n",
    "    # print column names\n",
    "    colnames = [\"Instance\"]\n",
    "    for j in range(1, 7):\n",
    "        colnames += [\n",
    "            f\"min{j}\", f\"max{j}\", f\"mean{j}\", f\"median{j}\",\n",
    "            f\"std{j}\", f\"1st_quart{j}\", f\"3rd_quart{j}\"\n",
    "        ]\n",
    "\n",
    "    return pd.DataFrame(all_features, columns=colnames)\n",
    "\n",
    "X_train_features = extract_time_features_with_instance(X_train)\n",
    "X_test_features  = extract_time_features_with_instance(X_test)\n",
    "\n",
    "print(\"Train feature table:\")\n",
    "print(X_train_features.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f1f8e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       feature   std_hat  ci_lower   ci_upper\n",
      "0         min1  8.794295  7.544201  10.035101\n",
      "1         max1  4.429182  3.257113   5.341745\n",
      "2        mean1  4.917717  4.316704   5.386538\n",
      "3      median1  4.956111  4.307741   5.435978\n",
      "4         std1  1.756796  1.537618   1.933586\n",
      "5   1st_quart1  5.731262  5.123539   6.139473\n",
      "6   3rd_quart1  4.783645  3.920147   5.480000\n",
      "7         min2  0.000000  0.000000   0.000000\n",
      "8         max2  5.147841  4.611797   5.488066\n",
      "9        mean2  1.600661  1.397702   1.732134\n",
      "10     median2  1.436903  1.235323   1.573123\n",
      "11        std2  0.901829  0.801854   0.955654\n",
      "12  1st_quart2  0.952201  0.818942   1.047242\n",
      "13  3rd_quart2  2.158258  1.889708   2.325299\n",
      "14        min3  3.053869  2.817137   3.200269\n",
      "15        max3  4.759853  3.949477   5.370447\n",
      "16       mean3  3.863304  3.186092   4.382022\n",
      "17     median3  3.845730  3.148523   4.403001\n",
      "18        std3  0.994970  0.770499   1.190774\n",
      "19  1st_quart3  4.145255  3.464232   4.671244\n",
      "20  3rd_quart3  3.946023  3.240262   4.498133\n",
      "21        min4  0.000000  0.000000   0.000000\n",
      "22        max4  2.302408  2.065030   2.483008\n",
      "23       mean4  1.179781  1.073037   1.229562\n",
      "24     median4  1.149585  1.042441   1.200294\n",
      "25        std4  0.473031  0.428802   0.500679\n",
      "26  1st_quart4  0.842771  0.759459   0.888142\n",
      "27  3rd_quart4  1.566564  1.421541   1.631514\n",
      "28        min5  5.368786  3.623438   6.817115\n",
      "29        max5  5.449726  4.284964   6.323261\n",
      "30       mean5  5.120513  3.703626   6.246384\n",
      "31     median5  5.267414  3.808317   6.468180\n",
      "32        std5  1.056870  0.793064   1.275628\n",
      "33  1st_quart5  5.543882  4.073381   6.757843\n",
      "34  3rd_quart5  4.957231  3.619118   6.021292\n",
      "35        min6  0.051766  0.000000   0.087690\n",
      "36        max6  2.540166  2.203765   2.808410\n",
      "37       mean6  1.171306  1.056591   1.228764\n",
      "38     median6  1.104626  0.996591   1.162707\n",
      "39        std6  0.518835  0.471795   0.544975\n",
      "40  1st_quart6  0.774233  0.693918   0.821478\n",
      "41  3rd_quart6  1.550218  1.400310   1.622121\n"
     ]
    }
   ],
   "source": [
    "# calculate standard deviation with bootstrap CI\n",
    "#i told the chatgpt that i need the loop outside and use the bootstrap to calculate the CI\n",
    "import pandas as pd\n",
    "from scipy.stats import bootstrap\n",
    "\n",
    "def bootstrap_std_ci_scipy(df, ci=0.90, n_resamples=2000, random_state=42):\n",
    "    results = []\n",
    "    for col in df.columns:\n",
    "        data = df[col].dropna().to_numpy()\n",
    "        if len(data) < 2:\n",
    "            results.append((col, np.nan, np.nan, np.nan))\n",
    "            continue\n",
    "\n",
    "        res = bootstrap(\n",
    "            (data,), \n",
    "            np.std,\n",
    "            confidence_level=ci,\n",
    "            n_resamples=n_resamples,\n",
    "            method=\"percentile\",\n",
    "            random_state=random_state\n",
    "        )\n",
    "        std_hat = np.std(data, ddof=1)\n",
    "        ci_low = res.confidence_interval.low\n",
    "        ci_high = res.confidence_interval.high\n",
    "        results.append((col, std_hat, ci_low, ci_high))\n",
    "\n",
    "    return pd.DataFrame(results, columns=[\"feature\", \"std_hat\", \"ci_lower\", \"ci_upper\"])\n",
    "\n",
    "ci90_df = bootstrap_std_ci_scipy(X_train_features.iloc[:, 1:], ci=0.90)\n",
    "print(ci90_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c292b425",
   "metadata": {},
   "source": [
    "#### 1(c)(iv) Choose Three Features\n",
    "I select: **min, mean, max**.\n",
    "\n",
    "- Min/Max capture the amplitude envelope and extrema, which differ across activities (e.g., dynamic motions like walking tend to have higher variability and broader extrema than static postures like sitting/lying).\n",
    "- Mean captures the central tendency (baseline level), which can shift across activities and sensors.\n",
    "\n",
    "These three are simple, robust, and complementary: extrema summarize range, mean summarizes level. They also tend to be less sensitive to small sample peculiarities than higher-order moments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68888084",
   "metadata": {},
   "source": [
    "## 2. Linear vs Cubic Regression Analysis\n",
    "\n",
    "I collect a set of data (n = 100 observations) containing a single predictor and a quantitative response. I then fit a linear regression model to the data, as well as a separate cubic regression, i.e. Y = β₀ + β₁X + β₂X² + β₃X³ + ϵ.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8f912640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "21bb6be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for data generation and model fitting\n",
    "def generate_data(n=100, true_relationship='linear', noise_level=1.0):\n",
    "    \"\"\"Generate data with specified true relationship\"\"\"\n",
    "    X = np.linspace(-2, 2, n).reshape(-1, 1)\n",
    "    \n",
    "    if true_relationship == 'linear':\n",
    "        y_true = 2 * X.flatten() + 1\n",
    "    elif true_relationship == 'quadratic':\n",
    "        y_true = X.flatten()**2 + 0.5 * X.flatten() + 1\n",
    "    elif true_relationship == 'cubic':\n",
    "        y_true = 0.5 * X.flatten()**3 - X.flatten()**2 + 2 * X.flatten() + 1\n",
    "    else:\n",
    "        y_true = np.sin(2 * X.flatten()) + 0.5 * X.flatten()\n",
    "    \n",
    "    # Add noise\n",
    "    y = y_true + noise_level * np.random.randn(n)\n",
    "    return X, y, y_true\n",
    "\n",
    "def fit_models(X, y):\n",
    "    \"\"\"Fit linear and cubic regression models\"\"\"\n",
    "    # Linear regression\n",
    "    linear_model = LinearRegression()\n",
    "    linear_model.fit(X, y)\n",
    "    y_pred_linear = linear_model.predict(X)\n",
    "    \n",
    "    # Cubic regression\n",
    "    poly_features = PolynomialFeatures(degree=3, include_bias=False)\n",
    "    X_poly = poly_features.fit_transform(X)\n",
    "    cubic_model = LinearRegression()\n",
    "    cubic_model.fit(X_poly, y)\n",
    "    y_pred_cubic = cubic_model.predict(X_poly)\n",
    "    \n",
    "    return linear_model, cubic_model, y_pred_linear, y_pred_cubic\n",
    "\n",
    "def calculate_rss(y_true, y_pred):\n",
    "    \"\"\"Calculate Residual Sum of Squares\"\"\"\n",
    "    return np.sum((y_true - y_pred)**2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a27f264",
   "metadata": {},
   "source": [
    "### 2(a) Training RSS when true relationship is linear\n",
    "\n",
    "**Question:** Suppose that the true relationship between X and Y is linear, i.e. Y = β₀ + β₁X + ϵ. Consider the training residual sum of squares (RSS) for the linear regression, and also the training RSS for the cubic regression. Would we expect one to be lower than the other, would we expect them to be the same, or is there not enough information to tell? Justify your answer.\n",
    "\n",
    "**Answer:** We would expect the **cubic regression to have lower or equal training RSS** compared to the linear regression.\n",
    "\n",
    "**Justification:**\n",
    "- The cubic model includes the linear model as a special case (when β₂ = β₃ = 0)\n",
    "- Since the cubic model has more parameters (4 vs 2), it has more flexibility to fit the training data\n",
    "- Even if the true relationship is linear, the cubic model can still fit the data perfectly by setting β₂ ≈ 0 and β₃ ≈ 0\n",
    "- The linear model is constrained to a linear relationship, while the cubic model can approximate any smooth function\n",
    "- Therefore: RSS_cubic ≤ RSS_linear (with equality only if the linear fit is already optimal)\n",
    "\n",
    "### 2(b) Test RSS when true relationship is linear\n",
    "\n",
    "**Question:** Answer (a) using test rather than training RSS.\n",
    "\n",
    "**Answer:** We would expect the **linear regression to have lower test RSS** compared to the cubic regression.\n",
    "\n",
    "**Justification:**\n",
    "- Since the true relationship is linear, the cubic model will likely overfit to the training data\n",
    "- The cubic model's additional parameters (β₂, β₃) will capture noise rather than true signal\n",
    "- On new test data, the overfitted cubic model will perform worse than the correctly specified linear model\n",
    "- The linear model is the correct specification, so it will generalize better to unseen data\n",
    "- Therefore: RSS_cubic_test > RSS_linear_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e1479057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Linear Relationship Demo ===\n",
      "Training RSS - Linear: 16.23, Cubic: 15.20\n",
      "Test RSS - Linear: 4.16, Cubic: 4.19\n",
      "Cubic ≤ Linear on training: True\n",
      "Linear < Cubic on test: True\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate with linear relationship (true relationship is linear)\n",
    "print(\"=== Linear Relationship Demo ===\")\n",
    "X, y, y_true = generate_data(n=100, true_relationship='linear', noise_level=0.5)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Fit models\n",
    "linear_model, cubic_model, y_pred_linear_train, y_pred_cubic_train = fit_models(X_train, y_train)\n",
    "\n",
    "# Test predictions\n",
    "y_pred_linear_test = linear_model.predict(X_test)\n",
    "poly_features = PolynomialFeatures(degree=3, include_bias=False)\n",
    "X_test_poly = poly_features.fit_transform(X_test)\n",
    "y_pred_cubic_test = cubic_model.predict(X_test_poly)\n",
    "\n",
    "# Calculate RSS\n",
    "rss_linear_train = calculate_rss(y_train, y_pred_linear_train)\n",
    "rss_cubic_train = calculate_rss(y_train, y_pred_cubic_train)\n",
    "rss_linear_test = calculate_rss(y_test, y_pred_linear_test)\n",
    "rss_cubic_test = calculate_rss(y_test, y_pred_cubic_test)\n",
    "\n",
    "print(f\"Training RSS - Linear: {rss_linear_train:.2f}, Cubic: {rss_cubic_train:.2f}\")\n",
    "print(f\"Test RSS - Linear: {rss_linear_test:.2f}, Cubic: {rss_cubic_test:.2f}\")\n",
    "print(f\"Cubic ≤ Linear on training: {rss_cubic_train <= rss_linear_train}\")\n",
    "print(f\"Linear < Cubic on test: {rss_linear_test < rss_cubic_test}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ea9722",
   "metadata": {},
   "source": [
    "### 2(c) Training RSS when true relationship is unknown\n",
    "\n",
    "**Question:** Suppose that the true relationship between X and Y is not linear, but we don't know how far it is from linear. Consider the training RSS for the linear regression, and also the training RSS for the cubic regression. Would we expect one to be lower than the other, would we expect them to be the same, or is there not enough information to tell? Justify your answer.\n",
    "\n",
    "**Answer:** We would expect the **cubic regression to have lower training RSS** compared to the linear regression.\n",
    "\n",
    "**Justification:**\n",
    "- The cubic model has more flexibility and can capture non-linear relationships\n",
    "- If the true relationship is non-linear, the linear model will be misspecified and have higher bias\n",
    "- The cubic model can approximate many smooth non-linear functions better than a linear model\n",
    "- With more parameters, the cubic model has more capacity to fit the training data\n",
    "- Therefore: RSS_cubic < RSS_linear (unless the relationship is actually linear)\n",
    "\n",
    "### 2(d) Test RSS when true relationship is unknown\n",
    "\n",
    "**Question:** Answer (c) using test rather than training RSS.\n",
    "\n",
    "**Answer:** **There is not enough information to tell** which will have lower test RSS.\n",
    "\n",
    "**Justification:**\n",
    "- The answer depends on the **bias-variance tradeoff**:\n",
    "  - **Linear model**: Higher bias (if relationship is non-linear) but lower variance\n",
    "  - **Cubic model**: Lower bias (can capture non-linear patterns) but higher variance\n",
    "- **If the relationship is close to linear**: Linear model wins (lower test RSS)\n",
    "- **If the relationship is highly non-linear**: Cubic model wins (lower test RSS)  \n",
    "- **If the relationship is moderately non-linear**: Depends on the specific form and noise level\n",
    "- **Sample size matters**: With n=100, the cubic model might overfit even if the relationship is non-linear\n",
    "- **Noise level matters**: High noise favors simpler models (linear), low noise favors more complex models (cubic)\n",
    "\n",
    "**Conclusion:** Without knowing the specific form of the non-linear relationship, we cannot definitively say which model will perform better on test data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a21c70a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Non-linear Relationship Demo ===\n",
      "Training RSS - Linear: 143.16, Cubic: 15.96\n",
      "Test RSS - Linear: 72.92, Cubic: 6.52\n",
      "Cubic < Linear on training: True\n",
      "Cubic < Linear on test: True\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate with non-linear relationship\n",
    "print(\"\\n=== Non-linear Relationship Demo ===\")\n",
    "X, y, y_true = generate_data(n=100, true_relationship='cubic', noise_level=0.5)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Fit models\n",
    "linear_model, cubic_model, y_pred_linear_train, y_pred_cubic_train = fit_models(X_train, y_train)\n",
    "\n",
    "# Test predictions\n",
    "y_pred_linear_test = linear_model.predict(X_test)\n",
    "poly_features = PolynomialFeatures(degree=3, include_bias=False)\n",
    "X_test_poly = poly_features.fit_transform(X_test)\n",
    "y_pred_cubic_test = cubic_model.predict(X_test_poly)\n",
    "\n",
    "# Calculate RSS\n",
    "rss_linear_train = calculate_rss(y_train, y_pred_linear_train)\n",
    "rss_cubic_train = calculate_rss(y_train, y_pred_cubic_train)\n",
    "rss_linear_test = calculate_rss(y_test, y_pred_linear_test)\n",
    "rss_cubic_test = calculate_rss(y_test, y_pred_cubic_test)\n",
    "\n",
    "print(f\"Training RSS - Linear: {rss_linear_train:.2f}, Cubic: {rss_cubic_train:.2f}\")\n",
    "print(f\"Test RSS - Linear: {rss_linear_test:.2f}, Cubic: {rss_cubic_test:.2f}\")\n",
    "print(f\"Cubic < Linear on training: {rss_cubic_train < rss_linear_train}\")\n",
    "print(f\"Cubic < Linear on test: {rss_cubic_test < rss_linear_test}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
